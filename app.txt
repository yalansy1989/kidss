# -*- coding: utf-8 -*-
# ÙˆØ§Ø¬Ø¨Ø§Øª-Ø¨ÙˆØª (Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠ) â€” ÙˆØ§Ø¬Ù‡Ø© Streamlit
# Kid Mode + Parent Mode + Ø¨Ø­Ø« ÙˆÙŠØ¨ + Ø­Ù„ Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù…Ø­Ù„ÙŠ
import re, html, requests
import streamlit as st
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
from rapidfuzz import fuzz
from sympy import sympify
from sympy.core.sympify import SympifyError

st.set_page_config(page_title="ÙˆØ§Ø¬Ø¨Ø§Øª-Ø¨ÙˆØª (Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠ)", page_icon="ğŸ§ ", layout="centered")

USER_AGENT = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
              "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36")

# ---------------- Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ----------------
def normalize_question(q: str) -> str:
    q = q.strip()
    trans = str.maketrans({
        "Ã—": "*", "Ã·": "/", "âˆ’": "-", "â€“": "-", "â€”": "-",
        "Ùª": "%", "ØŒ": ",", "ØŸ": "", "â€": "", "Ù€": ""
    })
    q = q.translate(trans)
    q = re.sub(r"\s+", " ", q)
    return q

def try_solve_math(q: str):
    """
    ÙŠÙƒØ´Ù Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ØªØ¹Ø¨ÙŠØ±Ø§Ù‹ Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹ Ø¨Ø³ÙŠØ·Ø§Ù‹ ÙˆÙŠØ­Ù„Ù‡.
    Ø£Ù…Ø«Ù„Ø©: 35 + 12ØŒ 7Ã—8ØŒ (12+3)*4ØŒ 50% Ù…Ù† 200
    """
    expr = q
    expr = re.sub(r"(\d+)\s*%(?:\s*Ù…Ù†)?\s*(\d+)", r"(\1/100)*\2", expr)
    cleaned = re.sub(r"[^0-9\+\-\*/\.\(\)\s]", "", expr)
    if len(re.findall(r"\d", cleaned)) >= 2 and re.search(r"[+\-*/]", cleaned):
        try:
            val = sympify(cleaned).evalf()
            s = f"{float(val):.6g}"
            steps = f"Ø­Ø³Ø¨Ù†Ø§ Ø§Ù„ØªØ¹Ø¨ÙŠØ±: {cleaned}"
            return s, steps
        except (SympifyError, Exception):
            return None, None
    return None, None

@st.cache_data(show_spinner=False)
def ddg_search(q: str, n=8):
    with DDGS() as ddgs:
        return list(ddgs.text(q, region="xa-ar", safesearch="Moderate",
                              timelimit="y", max_results=n))

def fetch_text(url: str) -> str:
    try:
        r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        blocks = soup.find_all(["p", "li", "h1", "h2", "h3"])
        text = " ".join(b.get_text(" ", strip=True) for b in blocks)
        text = re.sub(r"\s+", " ", text)
        return text[:12000]
    except Exception:
        return ""

def best_sentence_match(text: str, question: str):
    sents = re.split(r"(?<=[\.!\ØŸ\!])\s+|\n+", text)
    best = ""
    best_score = -1
    for s in sents:
        score = fuzz.token_set_ratio(s, question)
        if score > best_score:
            best_score, best = score, s
    return best.strip(), best_score

def parse_options(q: str):
    # ØµÙŠØºØ©: Ø§Ù„Ø³Ø¤Ø§Ù„ | Ø®ÙŠØ§Ø±1 | Ø®ÙŠØ§Ø±2 | Ø®ÙŠØ§Ø±3 ...
    parts = [p.strip() for p in q.split("|")]
    if len(parts) >= 3:
        return parts[0], parts[1:]
    return q, None

def choose_from_options(options, corpus: str):
    scored = []
    for opt in options:
        score = max(
            fuzz.token_set_ratio(opt, corpus),
            corpus.lower().count(opt.lower()) * 10
        )
        scored.append((opt, score))
    scored.sort(key=lambda x: x[1], reverse=True)
    return scored[0][0], scored

def solve(question_raw: str):
    q_norm = normalize_question(question_raw)
    q, options = parse_options(q_norm)

    # 1) Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù…Ø­Ù„ÙŠØ§Ù‹
    val, steps = try_solve_math(q)
    if val is not None and options is None:
        return {
            "answer": val, "confidence": 95, "method": "Ø­Ø³Ø§Ø¨ Ù…Ø¨Ø§Ø´Ø± (Ù…Ø­Ù„ÙŠ)",
            "explain": steps, "sources": []
        }

    # 2) Ø¨Ø­Ø« ÙˆÙŠØ¨
    serp = ddg_search(q, n=8)
    if not serp:
        return {
            "answer": "Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ«ÙˆÙ‚Ø©.",
            "confidence": 0, "method": "Ø¨Ø­Ø« ÙˆÙŠØ¨",
            "explain": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙƒØ§ÙÙŠØ©.", "sources": []
        }

    sources, corpus = [], []
    for item in serp[:5]:
        url = item.get("href") or item.get("url")
        title = item.get("title", "")
        body = html.unescape(item.get("body", ""))
        if url:
            text = fetch_text(url)
            if text and len(text) > 200:
                corpus.append(text)
                sources.append({"title": title, "url": url})
            elif body:
                corpus.append(body)

    big_text = "\n".join(corpus) if corpus else " ".join(
        html.unescape(x.get("body", "")) for x in serp
    )

    if not big_text.strip():
        top = serp[0]
        return {
            "answer": html.unescape(top.get("body", "Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©.")),
            "confidence": 40, "method": "Ù…Ù‚ØªØ·Ù Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«",
            "explain": "ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø› Ø£Ø¸Ù‡Ø±Øª Ø§Ù„Ù…Ù‚ØªØ·Ù Ø§Ù„Ø£Ø¹Ù„Ù‰.",
            "sources": [{"title": top.get("title", ""),
                         "url": top.get("href") or top.get("url", "")}]
        }

    sent, score = best_sentence_match(big_text, q)

    if options:
        pick, scored = choose_from_options(options, big_text)
        explain = "ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ù†ØµÙˆØµ Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n" + "\n".join(
            f"- {opt}: {int(sc)}" for opt, sc in scored
        )
        return {
            "answer": pick, "confidence": min(95, max(50, int(score))),
            "method": "Ø¨Ø­Ø« ÙˆÙŠØ¨ + ØªØ±Ø¬ÙŠØ­ Ø®ÙŠØ§Ø±Ø§Øª",
            "explain": explain, "sources": sources[:4]
        }

    return {
        "answer": sent if sent else "Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ø¬Ù…Ù„Ø© Ø­Ø§Ø³Ù…Ø©ØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±.",
        "confidence": min(95, max(45, int(score))),
        "method": "Ø¨Ø­Ø« ÙˆÙŠØ¨ + Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø©",
        "explain": "Ø§Ø®ØªØ±Ù†Ø§ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„.",
        "sources": sources[:4]
    }

# ---------------- ÙˆØ§Ø¬Ù‡Ø© Streamlit ----------------
st.markdown("<h1 style='text-align:center'>ğŸ§  ÙˆØ§Ø¬Ø¨Ø§Øª-Ø¨ÙˆØª (Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠ)</h1>", unsafe_allow_html=True)
mode = st.radio("Ø§Ù„ÙˆØ¶Ø¹:", ["ğŸ‘§ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø·ÙØ§Ù„ (Ù…Ø¨Ø³Ù‘Ø·)", "ğŸ§‘â€ğŸ’¼ ÙˆØ¶Ø¹ ÙˆÙ„ÙŠÙ‘ Ø§Ù„Ø£Ù…Ø± (ØªÙØµÙŠÙ„ÙŠ)"], index=0)

st.caption("Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù„ØªØ¹Ù„Ù‘Ù… Ù„Ø§ Ù„Ù„ØºØ´. Ø§Ù„ÙÙƒØ±Ø© Ù†Ø´Ø±Ø­ *ÙƒÙŠÙ* ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø­Ù„.")

with st.form("hw_form"):
    st.write("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ Ø¨Ù‡Ø°Ù‡ Ø§Ù„ØµÙŠØºØ©: Ø§Ù„Ø³Ø¤Ø§Ù„ | Ø®ÙŠØ§Ø±1 | Ø®ÙŠØ§Ø±2 | Ø®ÙŠØ§Ø±3 ...)")
    q = st.text_area("Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø§Ù„Ø¨", height=120, placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§ØŸ | Ø¨Ø±Ù„ÙŠÙ† | Ù…Ø¯Ø±ÙŠØ¯ | Ø¨Ø§Ø±ÙŠØ³ | Ø±ÙˆÙ…Ø§")
    submitted = st.form_submit_button("Ø­Ù„ âœ…")

if submitted and q.strip():
    with st.spinner("Ù†Ø¨Ø­Ø« ÙˆÙ†Ø­Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„â€¦"):
        result = solve(q)

    # Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    st.success(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:** {result['answer']}")
    st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©", f"{result['confidence']}%")
    st.info(f"Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©: {result['method']}")

    # ÙˆØ¶Ø¹ ÙˆÙ„ÙŠ Ø§Ù„Ø£Ù…Ø±: Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø´Ø±Ø­ + Ø§Ù„Ù…ØµØ§Ø¯Ø±
    if mode.endswith("ØªÙØµÙŠÙ„ÙŠ"):
        if result.get("explain"):
            st.write("**Ø§Ù„Ø´Ø±Ø­:**")
            st.code(result["explain"], language="text")
        if result.get("sources"):
            st.divider()
            st.write("**Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©):**")
            for s in result["sources"]:
                if s.get("url"):
                    st.markdown(f"- [{s.get('title','Ù…ØµØ¯Ø±')}]({s['url']})")
                else:
                    st.markdown(f"- {s.get('title','Ù…ØµØ¯Ø±')}")

# ÙÙˆØªØ± ØªØ±Ø¨ÙˆÙŠ Ù„Ø·ÙŠÙ
st.caption("ğŸ’¡ ØªÙ„Ù…ÙŠØ­ ØªØ±Ø¨ÙˆÙŠ: Ø§Ø³Ø£Ù„ Ø·ÙÙ„Ùƒ ÙŠØ´Ø±Ø­ Ù„Ùƒ Ø®Ø·ÙˆØ© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨ØµÙˆØªÙ‡ Ù‚Ø¨Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ 'Ø­Ù„'.")
